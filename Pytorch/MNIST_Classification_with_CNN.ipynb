{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Classification with CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OibZvIR6NpGr"
      },
      "outputs": [],
      "source": [
        "## 인공지능을 위한 수학 2 \n",
        "## 제 6강 : MNIST Classification using PyTorch\n",
        "\n",
        "\n",
        "## Module Import \n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "## Google Drive Mount \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/my_drive')\n",
        "\n",
        "\n",
        "## Trainig Setup\n",
        "\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\"*44}')\n",
        "\n",
        "\n",
        "## MNIST dataset Download \n",
        "train_dataset = datasets.MNIST(root='/content/my_drive/MyDrive/mai1/mnist_data/', \n",
        "                              train=True, \n",
        "                              transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "test_dataset = datasets.MNIST(root='/content/my_drive/MyDrive/mai1/mnist_data/',\n",
        "                             train=False,\n",
        "                             transform=transforms.ToTensor())\n",
        "\n",
        "## Data loader\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                               batch_size=batch_size,\n",
        "                               shuffle=True)\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=False)\n",
        "\n",
        "\n",
        "## Network Architecture Definition\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()     #사실 Net하고 self를 안 해줘도 잘 찾아가긴함\n",
        "    self.l1 = nn.Conv2d(1, 16, 3, padding=1)    #28x28x1 -> 28x28x16 -> 28x28x16 -> 28x28x32\n",
        "    self.l2 = nn.Linear(16, 16, 3, padding=1)\n",
        "    self.l3 = nn.Linear(16, 32, 3, padding=1)\n",
        "    self.l4 = nn.Linear(1568, 120)\n",
        "    self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.max_pool2d(F.relu(self.l1(x)), 2)\n",
        "    x = F.relu(self.l2(x))\n",
        "    x = F.max_pool2d(F.relu(self.l3(x)), 2)\n",
        "    x = x.view(-1, 1568)\n",
        "    x = F.relu(self.l4(x))\n",
        "    return self.l5(x)\n",
        "\n",
        "## Network Load\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "\n",
        "## Model Load\n",
        "\n",
        "# model.load_state_dict(torch.load(\"004_my_model.pt\"))   예를 들어 4번째까지 하고 멈췄을 때 이 weight를 가지고 시작하려고 할 때 사용\n",
        "\n",
        "\n",
        "## Training Setup\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "## Model Saving    이를 통해서 weight와 bias를 알 수 있다. model.state_dict() 안에 저장되어 있다.\n",
        "\n",
        "#for param_tensor in model.state_dict():\n",
        "#  print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size()) #이걸 출력하면 엄청난 크기의 weight들이 출력됨\n",
        "\n",
        "## Train Function \n",
        "\n",
        "def train(epoch):\n",
        "  model.train()    #train을 위한 형식\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()     #학습을 완료할 때마다 gradient들을 0으로 초기화 해줘야 한다.\n",
        "    output = model(data)      #output 계산\n",
        "    loss = criterion(output, target)     #loss 계산\n",
        "    loss.backward()     #편미분 값 계산\n",
        "    optimizer.step()     #weight update\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch : {} | Batch Status : {}/{} ({:.0f}%) | Loss : {:.6f}'.format(\n",
        "          epoch, batch_idx*len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    ## Save weight parameter\n",
        "    torch.save(model.state_dict(), \"%03d_my_model.pt\"%epoch)\n",
        "\n",
        "    #좀 더 세세하게 저장하려면 밑처럼\n",
        "    '''\n",
        "    torch.save({'epoch' : epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss\n",
        "                }, \"%03d_my_model.pt\"%epoch)\n",
        "    '''\n",
        "\n",
        "## Test Function\n",
        "\n",
        "def test():\n",
        "  model.eval()    #test를 위한 형식\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:      #답을 모를땐 target 부분이 존재하지 않는다. MNIST의 경우 답이 존재하므로 답 체크를 위해 써놓는 것\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "\n",
        "    # sum up batch loss\n",
        "    test_loss += criterion(output, target).item()\n",
        "\n",
        "    # get the index of the max\n",
        "    pred = output.data.max(1, keepdim=True)[1]\n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print(f'==================\\nTest set: Average loss : {test_loss:.4f}, Accuracy : {correct}/{len(test_loader.dataset)}'\n",
        "        f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "  \n",
        "\n",
        "## Main\n",
        "\n",
        "if __name__ == '__main__':\n",
        " \n",
        "  since = time.time()\n",
        "  for epoch in range(1, 10):\n",
        "    epoch_start = time.time()\n",
        "    train(epoch)\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "    \n",
        "    test()\n",
        "    m, s = divmod(time.time() - epoch_start, 60)\n",
        "    print(f'Tesing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "  m, s = divmod(time.time() - epoch_start, 60)\n",
        "  print(f'Total time : {m:.0f}m {s: .0f}s \\nModel was trained on {device}!')"
      ]
    }
  ]
}